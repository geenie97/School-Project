{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"web_site.ipynb","provenance":[],"collapsed_sections":["-Re6GFFWJXdH"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"s2SoQzLrYVYt"},"source":["#**멀티미디어 컴퓨팅**<Br>\n","\n","<br>\n","1. 코드를 모두 실행시킨다\n","2. flask code 부분에 있는 코드를 실행시키면 나오는 ngrok 링크(.io)에 접속한다. \n","3. 이후 "]},{"cell_type":"markdown","metadata":{"id":"LlQCR2UCJLyU"},"source":["# 라이브러리"]},{"cell_type":"code","metadata":{"id":"09uSlxI2JIPd"},"source":["import os\n","import gdown\n","import shutil \n","import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","import subprocess"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORzDXOQhHiZO","executionInfo":{"status":"ok","timestamp":1623134371067,"user_tz":-540,"elapsed":3811,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"33c86c4c-004d-4299-b389-98eed74e4cef"},"source":["!pip install flask_ngrok\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting flask_ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2020.12.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IA2eWUfNJINI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623134371068,"user_tz":-540,"elapsed":12,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"38698b17-ba45-4528-a036-b3621c39f06e"},"source":["os.mkdir('templates')\n","os.mkdir('static')\n","%cd static/\n","os.mkdir('anime_outputs')\n","os.mkdir('seg_outputs')\n","os.mkdir('splicing_outputs')\n","os.mkdir('inputs')\n","%cd /content/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/static\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0mXOsmptJOPQ"},"source":["# Github 불러오기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLHbz0LlJIKq","executionInfo":{"status":"ok","timestamp":1623134401416,"user_tz":-540,"elapsed":30357,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"887bd473-592a-40bd-af85-b33020ae61c6"},"source":["!git clone https://github.com/TachibanaYoshino/AnimeGANv2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'AnimeGANv2'...\n","remote: Enumerating objects: 1679, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 1679 (delta 0), reused 0 (delta 0), pack-reused 1676\u001b[K\n","Receiving objects: 100% (1679/1679), 996.82 MiB | 36.88 MiB/s, done.\n","Resolving deltas: 100% (110/110), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5mB_ca5DJIIR","executionInfo":{"status":"ok","timestamp":1623134475361,"user_tz":-540,"elapsed":73977,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"ac13c090-a4ff-40aa-e0ab-1c13b5489234"},"source":["!pip install tensorflow==1.15\n","!git clone https://github.com/bryandlee/animegan2-pytorch.git\n","%cd animegan2-pytorch\n","!mkdir inputs\n","!mkdir outputs\n","%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 35kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n","\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/keras-applications/\u001b[0m\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 41.6MB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 58.1MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.34.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (57.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=7040f1405c6eeb5aaa275687bd554fb42d9517e271f067780c1dacc6682ed583\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n","Cloning into 'animegan2-pytorch'...\n","remote: Enumerating objects: 86, done.\u001b[K\n","remote: Counting objects: 100% (86/86), done.\u001b[K\n","remote: Compressing objects: 100% (79/79), done.\u001b[K\n","remote: Total 86 (delta 31), reused 42 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects: 100% (86/86), done.\n","/content/animegan2-pytorch\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvi8UC2MJIGB","executionInfo":{"status":"ok","timestamp":1623134478980,"user_tz":-540,"elapsed":3638,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"f0c49412-0da3-4e4c-bf2f-b9e93404bbd8"},"source":["!pip install ninja\n","!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n","%cd Self-Correction-Human-Parsing\n","!mkdir checkpoints\n","!mkdir inputs\n","!mkdir outputs\n","%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting ninja\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n","\r\u001b[K     |███                             | 10kB 21.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 29.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 92kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102kB 14.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 14.9MB/s \n","\u001b[?25hInstalling collected packages: ninja\n","Successfully installed ninja-1.10.0.post2\n","Cloning into 'Self-Correction-Human-Parsing'...\n","remote: Enumerating objects: 719, done.\u001b[K\n","remote: Counting objects: 100% (11/11), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 719 (delta 3), reused 5 (delta 0), pack-reused 708\u001b[K\n","Receiving objects: 100% (719/719), 3.80 MiB | 19.43 MiB/s, done.\n","Resolving deltas: 100% (145/145), done.\n","/content/Self-Correction-Human-Parsing\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4SIm-f6JIDc","executionInfo":{"status":"ok","timestamp":1623134482945,"user_tz":-540,"elapsed":3973,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"c64d33ca-172d-4c4a-f811-c31f8f23d0ca"},"source":["!git clone https://github.com/menyifang/ADGAN.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'ADGAN'...\n","remote: Enumerating objects: 167, done.\u001b[K\n","remote: Counting objects: 100% (167/167), done.\u001b[K\n","remote: Compressing objects: 100% (141/141), done.\u001b[K\n","remote: Total 167 (delta 33), reused 153 (delta 22), pack-reused 0\u001b[K\n","Receiving objects: 100% (167/167), 18.67 MiB | 6.30 MiB/s, done.\n","Resolving deltas: 100% (33/33), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Re6GFFWJXdH"},"source":["# Style Transfer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhnaXCz-KS7M","executionInfo":{"status":"ok","timestamp":1623134482947,"user_tz":-540,"elapsed":33,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"0a19a4c0-4349-4ff9-ea49-38671025de13"},"source":["%cd /content\n","%cd animegan2-pytorch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","/content/animegan2-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CwBmDqocJIA-"},"source":["text = '''\n","#!/usr/bin/env python\n","# -*- encoding: utf-8 -*-\n","\n","import sys\n","sys.path.append('/content')\n","\n","import argparse\n","import numpy as np\n","import os\n","\n","import tensorflow as tf\n","from AnimeGANv2.net import generator as tf_generator\n","\n","import torch\n","from model import Generator\n","\n","\n","def load_tf_weights(tf_path):\n","    test_real = tf.placeholder(tf.float32, [1, None, None, 3], name='test')\n","    with tf.variable_scope(\"generator\", reuse=False):\n","        test_generated = tf_generator.G_net(test_real).fake\n","\n","    saver = tf.train.Saver()\n","\n","    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, device_count = {'GPU': 0})) as sess:\n","        ckpt = tf.train.get_checkpoint_state(tf_path)\n","\n","        assert ckpt is not None and ckpt.model_checkpoint_path is not None, f\"Failed to load checkpoint {tf_path}\"\n","\n","        saver.restore(sess, ckpt.model_checkpoint_path)\n","        print(f\"Tensorflow model checkpoint {ckpt.model_checkpoint_path} loaded\")\n","\n","        tf_weights = {}\n","        for v in tf.trainable_variables():\n","            tf_weights[v.name] = v.eval()\n","    \n","    return tf_weights\n","\n","            \n","def convert_keys(k):\n","\n","    # 1. divide tf weight name in three parts [block_idx, layer_idx, weight/bias]\n","    # 2. handle each part & merge into a pytorch model keys\n","    \n","    k = k.replace(\"Conv/\", \"Conv_0/\").replace(\"LayerNorm/\", \"LayerNorm_0/\")    \n","    keys = k.split(\"/\")[2:]\n","    \n","    is_dconv = False\n","\n","    # handle C block..\n","    if keys[0] == \"C\":\n","        if keys[1] in [\"Conv_1\", \"LayerNorm_1\"]:\n","            keys[1] = keys[1].replace(\"1\", \"5\")\n","        \n","        if len(keys) == 4:\n","            assert \"r\" in keys[1]\n","\n","            if keys[1] == keys[2]:\n","                is_dconv = True\n","                keys[2] = \"1.1\"\n","            \n","            block_c_maps = {\n","                \"1\":  \"1.2\",\n","                \"Conv_1\":  \"2\",\n","                \"2\":  \"3\",\n","            }\n","            if keys[2] in block_c_maps:\n","                keys[2] = block_c_maps[keys[2]]\n","\n","            keys[1] = keys[1].replace(\"r\", \"\") + \".layers.\" + keys[2]\n","            keys[2] = keys[3]\n","            keys.pop(-1)\n","    assert len(keys) == 3\n","\n","    # handle output block\n","    if \"out\" in keys[0]:\n","        keys[1] = \"0\"\n","    \n","    # first part\n","    if keys[0] in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n","        keys[0] = \"block_\" + keys[0].lower()        \n","        \n","    # second part\n","    if \"LayerNorm_\" in keys[1]:\n","        keys[1] = keys[1].replace(\"LayerNorm_\", \"\") + \".2\"\n","    if \"Conv_\" in keys[1]:\n","        keys[1] = keys[1].replace(\"Conv_\", \"\") + \".1\"\n","        \n","    # third part\n","    keys[2] = {\n","        \"weights:0\": \"weight\",\n","        \"w:0\": \"weight\",\n","        \"bias:0\": \"bias\",\n","        \"gamma:0\": \"weight\",\n","        \"beta:0\": \"bias\",\n","    }[keys[2]]\n","        \n","    return \".\".join(keys), is_dconv\n","\n","\n","def convert_and_save(tf_checkpoint_path, save_name):\n","\n","    tf_weights = load_tf_weights(tf_checkpoint_path)\n","    \n","    torch_net = Generator()\n","    torch_weights = torch_net.state_dict()\n","\n","    torch_converted_weights = {}\n","    for k, v in tf_weights.items():\n","        torch_k, is_dconv = convert_keys(k)\n","        assert torch_k in torch_weights, f\"weight name mismatch: {k}\"\n","\n","        converted_weight = torch.from_numpy(v)\n","        if len(converted_weight.shape) == 4:\n","            if is_dconv:\n","                converted_weight = converted_weight.permute(2, 3, 0, 1)\n","            else:\n","                converted_weight = converted_weight.permute(3, 2, 0, 1)\n","\n","        assert torch_weights[torch_k].shape == converted_weight.shape, f\"shape mismatch: {k}\"\n","\n","        torch_converted_weights[torch_k] = converted_weight\n","\n","    assert sorted(list(torch_converted_weights)) == sorted(list(torch_weights)), f\"some weights are missing\"\n","    torch_net.load_state_dict(torch_converted_weights)    \n","    torch.save(torch_net.state_dict(), save_name)\n","    print(f\"PyTorch model saved at {save_name}\")\n","    \n","\n","if __name__ == '__main__':\n","\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        '--tf_checkpoint_path',\n","        type=str,\n","        default='AnimeGANv2/checkpoint/generator_Paprika_weight',\n","    )\n","    parser.add_argument(\n","        '--save_name', \n","        type=str, \n","        default='pytorch_generator_Paprika.pt',\n","    )\n","    args = parser.parse_args()\n","    \n","    convert_and_save(args.tf_checkpoint_path, args.save_name)\n","'''\n","file = open(\"convert_weights2.py\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jOeOHXhOuQv"},"source":["# test code 만들기\n","text = '''\n","import sys\n","sys.path.append('/content/animegan2-pytorch')\n","\n","import torch\n","import cv2\n","import numpy as np\n","import os\n","\n","from model import Generator\n","import easydict\n","\n","torch.backends.cudnn.enabled = False\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","def load_image(image_path, x32=False):\n","    img = cv2.imread(image_path).astype(np.float32)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    h, w = img.shape[:2]\n","\n","    if x32:  # resize image to multiple of 32s\n","        def to_32s(x):\n","            return 256 if x < 256 else x - x % 32\n","\n","        img = cv2.resize(img, (to_32s(w), to_32s(h)))\n","\n","    img = torch.from_numpy(img)\n","    img = img / 127.5 - 1.0\n","    return img\n","\n","\n","def test(args):\n","    device = args.device\n","\n","    net = Generator()\n","    net.load_state_dict(torch.load(args.checkpoint, map_location=\"cpu\"))\n","    net.to(device).eval()\n","    print(f\"model loaded: {args.checkpoint}\")\n","\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    for image_name in sorted(os.listdir(args.input_dir)):\n","        if os.path.splitext(image_name)[-1].lower() not in [\".jpg\", \".png\", \".bmp\", \".tiff\"]:\n","            continue\n","\n","        image = load_image(os.path.join(args.input_dir, image_name), args.x32)\n","\n","        with torch.no_grad():\n","            input = image.permute(2, 0, 1).unsqueeze(0).to(device)\n","            out = net(input, args.upsample_align).squeeze(0).permute(1, 2, 0).cpu().numpy()\n","            out = (out + 1) * 127.5\n","            out = np.clip(out, 0, 255).astype(np.uint8)\n","\n","        cv2.imwrite(os.path.join(args.output_dir, image_name), cv2.cvtColor(out, cv2.COLOR_BGR2RGB))\n","        print(f\"image saved: {image_name}\")\n","\n","\n","if __name__ == '__main__':\n","\n","    args = easydict.EasyDict({\n","\n","        \"checkpoint\": '/content/pytorch_generator_Hayao.pt',\n","\n","        \"input_dir\": '/content/static/inputs',\n","\n","        \"output_dir\": '/content/static/anime_outputs',\n","\n","        \"device\": 'cpu',\n","\n","        \"upsample_align\": False,\n","\n","        \"x32\": \"store_true\"\n","\n","    })\n","\n","\n","    test(args)\n","\n","'''\n","file = open(\"anime_execute.py\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDu4jorpJH-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623134491154,"user_tz":-540,"elapsed":8234,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"31f91a97-ae10-4082-fa14-bece2cc7eb86"},"source":["%cd /content\n","! python /content/animegan2-pytorch/convert_weights2.py --tf_checkpoint_path '/content/AnimeGANv2/checkpoint/generator_Hayao_weight' --save_name 'pytorch_generator_Hayao.pt'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:20: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:21: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /content/AnimeGANv2/net/generator.py:41: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/AnimeGANv2/net/generator.py:58: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:24: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:26: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-06-08 06:41:28.406735: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n","2021-06-08 06:41:28.411601: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-06-08 06:41:28.411854: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e70b4ff480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-06-08 06:41:28.411885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-06-08 06:41:28.413695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-06-08 06:41:28.600004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-06-08 06:41:28.600986: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e70b4ff100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-06-08 06:41:28.601029: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-06-08 06:41:28.602058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-06-08 06:41:28.602084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n","Tensorflow model checkpoint /content/AnimeGANv2/checkpoint/generator_Hayao_weight/Hayao-99.ckpt loaded\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:35: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","PyTorch model saved at pytorch_generator_Hayao.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ft9Ix8viCrcA"},"source":["# paprika version"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g59pDgK-CrOj","executionInfo":{"status":"ok","timestamp":1623134491156,"user_tz":-540,"elapsed":38,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"eaeaf47c-c530-4c59-be57-a37986e28cf1"},"source":["%cd /content\n","%cd animegan2-pytorch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","/content/animegan2-pytorch\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7_TjVbRkCuoi"},"source":["# test code 만들기\n","text = '''\n","import sys\n","sys.path.append('/content/animegan2-pytorch')\n","\n","import torch\n","import cv2\n","import numpy as np\n","import os\n","\n","from model import Generator\n","import easydict\n","\n","torch.backends.cudnn.enabled = False\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","def load_image(image_path, x32=False):\n","    img = cv2.imread(image_path).astype(np.float32)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    h, w = img.shape[:2]\n","\n","    if x32:  # resize image to multiple of 32s\n","        def to_32s(x):\n","            return 256 if x < 256 else x - x % 32\n","\n","        img = cv2.resize(img, (to_32s(w), to_32s(h)))\n","\n","    img = torch.from_numpy(img)\n","    img = img / 127.5 - 1.0\n","    return img\n","\n","\n","def test(args):\n","    device = args.device\n","\n","    net = Generator()\n","    net.load_state_dict(torch.load(args.checkpoint, map_location=\"cpu\"))\n","    net.to(device).eval()\n","    print(f\"model loaded: {args.checkpoint}\")\n","\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    for image_name in sorted(os.listdir(args.input_dir)):\n","        if os.path.splitext(image_name)[-1].lower() not in [\".jpg\", \".png\", \".bmp\", \".tiff\"]:\n","            continue\n","\n","        image = load_image(os.path.join(args.input_dir, image_name), args.x32)\n","\n","        with torch.no_grad():\n","            input = image.permute(2, 0, 1).unsqueeze(0).to(device)\n","            out = net(input, args.upsample_align).squeeze(0).permute(1, 2, 0).cpu().numpy()\n","            out = (out + 1) * 127.5\n","            out = np.clip(out, 0, 255).astype(np.uint8)\n","\n","        cv2.imwrite(os.path.join(args.output_dir, image_name), cv2.cvtColor(out, cv2.COLOR_BGR2RGB))\n","        print(f\"image saved: {image_name}\")\n","\n","\n","if __name__ == '__main__':\n","\n","    args = easydict.EasyDict({\n","\n","        \"checkpoint\": '/content/pytorch_generator_Paprika.pt',\n","\n","        \"input_dir\": '/content/static/inputs',\n","\n","        \"output_dir\": '/content/static/anime_outputs',\n","\n","        \"device\": 'cpu',\n","\n","        \"upsample_align\": False,\n","\n","        \"x32\": \"store_true\"\n","\n","    })\n","\n","\n","    test(args)\n","\n","'''\n","file = open(\"anime_execute2.py\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWhCXtRrCiOH","executionInfo":{"status":"ok","timestamp":1623134495918,"user_tz":-540,"elapsed":4794,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"ab359616-2f5a-48bc-98c0-d0bb476ca9d3"},"source":["%cd /content\n","! python /content/animegan2-pytorch/convert_weights2.py "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:20: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:21: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /content/AnimeGANv2/net/generator.py:41: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/AnimeGANv2/net/generator.py:58: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:24: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:26: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-06-08 06:41:34.113208: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n","2021-06-08 06:41:34.118208: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-06-08 06:41:34.118484: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d107a79480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-06-08 06:41:34.118513: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-06-08 06:41:34.120198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-06-08 06:41:34.229069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-06-08 06:41:34.229983: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d107a79100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-06-08 06:41:34.230019: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-06-08 06:41:34.230117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-06-08 06:41:34.230156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n","Tensorflow model checkpoint AnimeGANv2/checkpoint/generator_Paprika_weight/Paprika-54.ckpt loaded\n","WARNING:tensorflow:From /content/animegan2-pytorch/convert_weights2.py:35: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","PyTorch model saved at pytorch_generator_Paprika.pt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v3ll2lAjS0mK"},"source":["#인물사진 segmentation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfxyZeESS59i","executionInfo":{"status":"ok","timestamp":1623134495920,"user_tz":-540,"elapsed":25,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"f52afcf2-c1b7-4510-8ea0-5347b2aa00b0"},"source":["%cd /content\n","%cd Self-Correction-Human-Parsing"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","/content/Self-Correction-Human-Parsing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"du_xQF6AS56_","executionInfo":{"status":"ok","timestamp":1623134502358,"user_tz":-540,"elapsed":6460,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"6f7c84b2-18d1-4f0c-907e-16da8e3704bc"},"source":["dataset = 'atr'         #select from ['lip', 'atr', 'pascal']\n","\n","if dataset == 'lip':\n","    url = 'https://drive.google.com/uc?id=1k4dllHpu0bdx38J7H28rVVLpU-kOHmnH'\n","elif dataset == 'atr':\n","    url = 'https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP'\n","elif dataset == 'pascal':\n","    url = 'https://drive.google.com/uc?id=1E5YwNKW2VOEayK9mWCS3Kpsxf-3z04ZE'\n","\n","output = 'checkpoints/final.pth'\n","gdown.download(url, output, quiet=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP\n","To: /content/Self-Correction-Human-Parsing/checkpoints/final.pth\n","267MB [00:03, 81.7MB/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'checkpoints/final.pth'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"iuzf3qjTS55G"},"source":["text = '''\n","\n","#!/usr/bin/env python\n","# -*- encoding: utf-8 -*-\n","\n","import os\n","import torch\n","import easydict\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","\n","import networks\n","from utils.transforms import transform_logits\n","from datasets.simple_extractor_dataset import SimpleFolderDataset\n","\n","dataset_settings = {\n","    'lip': {\n","        'input_size': [473, 473],\n","        'num_classes': 20,\n","        'label': ['Background', 'Hat', 'Hair', 'Glove', 'Sunglasses', 'Upper-clothes', 'Dress', 'Coat',\n","                  'Socks', 'Pants', 'Jumpsuits', 'Scarf', 'Skirt', 'Face', 'Left-arm', 'Right-arm',\n","                  'Left-leg', 'Right-leg', 'Left-shoe', 'Right-shoe']\n","    },\n","    'atr': {\n","        'input_size': [512, 512],\n","        'num_classes': 18,\n","        'label': ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt',\n","                  'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n","    },\n","    'pascal': {\n","        'input_size': [512, 512],\n","        'num_classes': 7,\n","        'label': ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs'],\n","    }\n","}\n","\n","\n","def get_arguments():\n","    \"\"\"Parse all the arguments provided from the CLI.\n","    Returns:\n","      A list of parsed arguments.\n","    \"\"\"\n","    args = easydict.EasyDict({\n","        \"dataset\": 'atr',\n","        \"model_restore\": 'checkpoints/final.pth',\n","        \"gpu\": '0',\n","        \"input_dir\": 'inputs',\n","        \"output_dir\": '/content/static/seg_outputs',\n","        \"logits\": False,\n","    })\n","    return args\n","\n","\n","def get_palette(num_cls):\n","    \"\"\" Returns the color map for visualizing the segmentation mask.\n","    Args:\n","        num_cls: Number of classes\n","    Returns:\n","        The color map\n","    \"\"\"\n","    n = num_cls\n","    palette = [0] * (n * 3)\n","    for j in range(0, n):\n","        lab = j\n","        palette[j * 3 + 0] = 0\n","        palette[j * 3 + 1] = 0\n","        palette[j * 3 + 2] = 0\n","        i = 0\n","        while lab:\n","            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n","            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n","            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n","            i += 1\n","            lab >>= 3\n","    return palette\n","\n","\n","def main():\n","    args = get_arguments()\n","\n","    gpus = [int(i) for i in args.gpu.split(',')]\n","    assert len(gpus) == 1\n","    if not args.gpu == 'None':\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n","\n","    num_classes = dataset_settings[args.dataset]['num_classes']\n","    input_size = dataset_settings[args.dataset]['input_size']\n","    label = dataset_settings[args.dataset]['label']\n","    print(\"Evaluating total class number {} with {}\".format(num_classes, label))\n","\n","    model = networks.init_model('resnet101', num_classes=num_classes, pretrained=None)\n","\n","    state_dict = torch.load(args.model_restore)['state_dict']\n","    from collections import OrderedDict\n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = k[7:]  # remove `module.`\n","        new_state_dict[name] = v\n","    model.load_state_dict(new_state_dict)\n","    model.cuda()\n","    model.eval()\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.406, 0.456, 0.485], std=[0.225, 0.224, 0.229])\n","    ])\n","    dataset = SimpleFolderDataset(root=args.input_dir, input_size=input_size, transform=transform)\n","    dataloader = DataLoader(dataset)\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    palette = [255, 255, 255,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0,\n","                0,0,0, ]\n","    with torch.no_grad():\n","        for idx, batch in enumerate(tqdm(dataloader)):\n","            image, meta = batch\n","            img_name = meta['name'][0]\n","            c = meta['center'].numpy()[0]\n","            s = meta['scale'].numpy()[0]\n","            w = meta['width'].numpy()[0]\n","            h = meta['height'].numpy()[0]\n","\n","            output = model(image.cuda())\n","            upsample = torch.nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n","            upsample_output = upsample(output[0][-1][0].unsqueeze(0))\n","            upsample_output = upsample_output.squeeze()\n","            upsample_output = upsample_output.permute(1, 2, 0)  # CHW -> HWC\n","\n","            logits_result = transform_logits(upsample_output.data.cpu().numpy(), c, s, w, h, input_size=input_size)\n","            parsing_result = np.argmax(logits_result, axis=2)\n","            parsing_result_path = os.path.join(args.output_dir, img_name[:-4] + '.png')\n","            output_img = Image.fromarray(np.asarray(parsing_result, dtype=np.uint8))\n","            output_img.putpalette(palette)\n","            output_img.save(parsing_result_path)\n","            if args.logits:\n","                logits_result_path = os.path.join(args.output_dir, img_name[:-4] + '.npy')\n","                np.save(logits_result_path, logits_result)\n","    return\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n","\n","'''\n","file = open(\"simple_extractor_edit_b.py\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgFG3S-uS5zr"},"source":["text = '''\n","\n","#!/usr/bin/env python\n","# -*- encoding: utf-8 -*-\n","\n","import sys\n","sys.path.append('/content/Self-Correction-Human-Parsing')\n","\n","import os\n","import torch\n","import easydict\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","\n","import networks\n","from utils.transforms import transform_logits\n","from datasets.simple_extractor_dataset import SimpleFolderDataset\n","\n","dataset_settings = {\n","    'lip': {\n","        'input_size': [473, 473],\n","        'num_classes': 20,\n","        'label': ['Background', 'Hat', 'Hair', 'Glove', 'Sunglasses', 'Upper-clothes', 'Dress', 'Coat',\n","                  'Socks', 'Pants', 'Jumpsuits', 'Scarf', 'Skirt', 'Face', 'Left-arm', 'Right-arm',\n","                  'Left-leg', 'Right-leg', 'Left-shoe', 'Right-shoe']\n","    },\n","    'atr': {\n","        'input_size': [512, 512],\n","        'num_classes': 18,\n","        'label': ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt',\n","                  'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n","    },\n","    'pascal': {\n","        'input_size': [512, 512],\n","        'num_classes': 7,\n","        'label': ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs'],\n","    }\n","}\n","\n","\n","def get_arguments():\n","    \"\"\"Parse all the arguments provided from the CLI.\n","    Returns:\n","      A list of parsed arguments.\n","    \"\"\"\n","    args = easydict.EasyDict({\n","        \"dataset\": 'atr',\n","        \"model_restore\": 'checkpoints/final.pth',\n","        \"gpu\": '0',\n","        \"input_dir\": 'inputs',\n","        \"output_dir\": '/content/static/seg_outputs',\n","        \"logits\": False,\n","    })\n","    return args\n","\n","\n","def get_palette(num_cls):\n","    \"\"\" Returns the color map for visualizing the segmentation mask.\n","    Args:\n","        num_cls: Number of classes\n","    Returns:\n","        The color map\n","    \"\"\"\n","    n = num_cls\n","    palette = [0] * (n * 3)\n","    for j in range(0, n):\n","        lab = j\n","        palette[j * 3 + 0] = 0\n","        palette[j * 3 + 1] = 0\n","        palette[j * 3 + 2] = 0\n","        i = 0\n","        while lab:\n","            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n","            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n","            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n","            i += 1\n","            lab >>= 3\n","    return palette\n","\n","\n","def main():\n","    args = get_arguments()\n","\n","    gpus = [int(i) for i in args.gpu.split(',')]\n","    assert len(gpus) == 1\n","    if not args.gpu == 'None':\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n","\n","    num_classes = dataset_settings[args.dataset]['num_classes']\n","    input_size = dataset_settings[args.dataset]['input_size']\n","    label = dataset_settings[args.dataset]['label']\n","    print(\"Evaluating total class number {} with {}\".format(num_classes, label))\n","\n","    model = networks.init_model('resnet101', num_classes=num_classes, pretrained=None)\n","\n","    state_dict = torch.load(args.model_restore)['state_dict']\n","    from collections import OrderedDict\n","    new_state_dict = OrderedDict()\n","    for k, v in state_dict.items():\n","        name = k[7:]  # remove `module.`\n","        new_state_dict[name] = v\n","    model.load_state_dict(new_state_dict)\n","    model.cuda()\n","    model.eval()\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.406, 0.456, 0.485], std=[0.225, 0.224, 0.229])\n","    ])\n","    dataset = SimpleFolderDataset(root=args.input_dir, input_size=input_size, transform=transform)\n","    dataloader = DataLoader(dataset)\n","\n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","\n","    palette = [0, 0, 0,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255,\n","                255, 255, 255, ]\n","    with torch.no_grad():\n","        for idx, batch in enumerate(tqdm(dataloader)):\n","            image, meta = batch\n","            img_name = meta['name'][0]\n","            c = meta['center'].numpy()[0]\n","            s = meta['scale'].numpy()[0]\n","            w = meta['width'].numpy()[0]\n","            h = meta['height'].numpy()[0]\n","\n","            output = model(image.cuda())\n","            upsample = torch.nn.Upsample(size=input_size, mode='bilinear', align_corners=True)\n","            upsample_output = upsample(output[0][-1][0].unsqueeze(0))\n","            upsample_output = upsample_output.squeeze()\n","            upsample_output = upsample_output.permute(1, 2, 0)  # CHW -> HWC\n","\n","            logits_result = transform_logits(upsample_output.data.cpu().numpy(), c, s, w, h, input_size=input_size)\n","            parsing_result = np.argmax(logits_result, axis=2)\n","            parsing_result_path = os.path.join(args.output_dir, img_name[:-4] + '.png')\n","            output_img = Image.fromarray(np.asarray(parsing_result, dtype=np.uint8))\n","            output_img.putpalette(palette)\n","            output_img.save(parsing_result_path)\n","            if args.logits:\n","                logits_result_path = os.path.join(args.output_dir, img_name[:-4] + '.npy')\n","                np.save(logits_result_path, logits_result)\n","    return\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n","'''\n","file = open(\"simple_extractor_edit_w.py\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mcsA5k7agez"},"source":["def segmentation(file_name):\n","    ## segmentation\n","  src = '/content/static/anime_outputs/' + file_name\n","  dst = '/content/Self-Correction-Human-Parsing/inputs'\n","  shutil.copy(src, dst)\n","\n","  os.chdir('/content/Self-Correction-Human-Parsing')\n","\n","  try:\n","    subprocess.call(['python', '/content/Self-Correction-Human-Parsing/simple_extractor_edit_b.py']) # 사람 까맣게\n","  except:\n","    print('error')\n","\n","  os.rename('/content/static/seg_outputs/' + file_name.split('.')[0] + '.png', '/content/static/seg_outputs/' + file_name.split('.')[0] + '_black.png')\n","\n","  try:\n","    subprocess.call(['python', '/content/Self-Correction-Human-Parsing/simple_extractor_edit_w.py']) # 사람 까맣게\n","  except:\n","    print('error')\n","\n","  os.rename('/content/static/seg_outputs/' + file_name.split('.')[0] + '.png', '/content/static/seg_outputs/' + file_name.split('.')[0] + '_white.png')\n","\n","  os.chdir('/content')\n","  ## finish segmentation\n","\n","  return \"segmentation finished\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuwwOYWyZoLO"},"source":["# 이미지 전처리"]},{"cell_type":"code","metadata":{"id":"l8E8X7r_4ur0"},"source":["white_img = np.full((256, 256), 255, dtype=np.uint8)\n","black_img = np.full((256, 256), 0, dtype=np.uint8)\n","color_img = np.full((256, 256, 3), (0, 255, 255), dtype=np.uint8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcFH4T4jZWVp"},"source":["def pre_processing(black_mask, ani_img): # 이미지 경로 말고 이미지 이름 쓰는 거 임\n","  src = cv2.imread('/content/static/seg_outputs/' + black_mask, cv2.IMREAD_COLOR)        # 컬러 영상\n","  mask = cv2.imread('/content/static/seg_outputs/' + black_mask, cv2.IMREAD_GRAYSCALE)         # 그레이스케일 영상\n","  dst = cv2.imread('/content/static/anime_outputs/' + ani_img, cv2.IMREAD_COLOR)\n","\n","  cv2.copyTo(src, mask, dst)\n","  cv2.imwrite('/content/static/seg_outputs/'+ ani_img.split('.')[0] + '_white_back.jpg', dst) # 이 이미지 보여주기\n","\n","  return 'preprocessing OK', '/static/seg_outputs/'+ ani_img.split('.')[0] + '_white_back.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifdrpuiKj8Ps"},"source":["def make_paddding(file_path):\n","  img = cv2.imread(file_path)\n","  a, b, c = img.shape\n","\n","  differ = b - a\n","  differ_1, differ_2 = 0, 0\n","\n","  if differ%2 == 0:\n","    differ_1, differ_2 = abs(differ/2), abs(differ/2)\n","  else:\n","    differ_1, differ_2 = abs(differ/2), abs(differ/2) + 1\n","\n","  change_img = cv2.copyMakeBorder(img, 0, 0, int(differ_1), int(differ_2), cv2.BORDER_CONSTANT, value=[255, 255, 255])\n","  cv2.imwrite(file_path, change_img)\n","\n","  return \"make padding OK\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_iM6JkRFbrQ_"},"source":["# 이미지 합성하기"]},{"cell_type":"code","metadata":{"id":"AijwUsKeZWYO"},"source":["def splicing_back(idx, back_file_path, human_file):\n","  back = cv2.imread(back_file_path, cv2.IMREAD_COLOR)        # 컬러 영상\n","  back_resize = cv2.resize(back, dsize=(256*4, 256*2+60), interpolation=cv2.INTER_AREA)\n","  #cv2_imshow(back_resize)\n","\n","  sp_idx = idx\n","  temp = cv2.imread('/content/static/seg_outputs/' + human_file.split('.')[0] + '_white.png', cv2.IMREAD_GRAYSCALE) \n","  temp2 = cv2.imread('/content/Self-Correction-Human-Parsing/inputs/'+human_file, cv2.IMREAD_COLOR)\n","\n","\n","  print(back_resize.shape, \" \", temp.shape, \" \", temp2.shape)\n","\n","\n","\n","  black_4 = cv2.hconcat([black_img, black_img, black_img, black_img])\n","  color_4 = cv2.hconcat([color_img, color_img, color_img, color_img])\n","\n","  if sp_idx == 1 or sp_idx == 5:\n","    black_mask = cv2.hconcat([temp, black_img, black_img, black_img])\n","    color_mask = cv2.hconcat([temp2, color_img, color_img, color_img])\n","  elif sp_idx == 2 or sp_idx == 6:\n","    black_mask = cv2.hconcat([black_img, temp, black_img, black_img])\n","    color_mask = cv2.hconcat([color_img, temp2, color_img, color_img])\n","  elif sp_idx == 3 or sp_idx == 7:\n","    black_mask = cv2.hconcat([black_img, black_img, temp, black_img])\n","    color_mask = cv2.hconcat([color_img, color_img, temp2, color_img])\n","  elif sp_idx == 4 or sp_idx == 8:\n","    black_mask = cv2.hconcat([black_img, black_img, black_img, temp])\n","    color_mask = cv2.hconcat([color_img, color_img, color_img, temp2])\n","\n","  if sp_idx > 4:\n","    back_mask = cv2.vconcat([black_4, black_mask])\n","    human_mask = cv2.vconcat([color_4, color_mask])\n","  else:\n","    back_mask = cv2.vconcat([black_mask, black_4])\n","    human_mask = cv2.vconcat([color_mask, color_4])\n","\n","  #cv2_imshow(human_mask)\n","\n","  black_edge = np.full((30, 1024), 0, dtype=np.uint8)\n","  color_edge = np.full((30, 1024, 3), (0, 255, 255), dtype=np.uint8)\n","\n","  adflkajsdf = cv2.vconcat([back_mask, black_edge])\n","  back_mask = cv2.vconcat([black_edge, adflkajsdf])\n","  #cv2_imshow(back_mask)\n","\n","  adkfalkdfd = cv2.vconcat([human_mask, color_edge])\n","  human_mask = cv2.vconcat([color_edge, adkfalkdfd])\n","  #cv2_imshow(human_mask)\n","  #print(back_mask.shape, back_resize.shape, human_mask.shape)\n","\n","  cv2.copyTo(human_mask, back_mask, back_resize)\n","  cv2.imwrite('/content/static/splicing_outputs/splicing_result.jpg', back_resize)\n","\n","  return \"finish splicing\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-ARxBg-GtrG"},"source":["# 웹페이지(HTML)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_7p8iTM1rUA","executionInfo":{"status":"ok","timestamp":1623134502370,"user_tz":-540,"elapsed":50,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"0a9ff027-4feb-4883-d55c-f261dee73beb"},"source":["%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"54UAl0CeZWaa"},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Multi-Media Computing Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","</head>\n","\n","    <body>\n","    <p><h1 align=\"center\">Multi-Media Computing Website</h1></p>\n","\n","<div class=\"container\">\n","<div class=\"row\">\n","    <h2>Select a file to upload</h2>\n","    <h5>합성하고자 하는 배경 사진과 인물 사진을 업로드 해 주세요.</h5>\n","\n","\n","     <form action = \"/fileUpload\" method = \"POST\" enctype = \"multipart/form-data\">\n","        <dl>\n","            <p>\n","                <input type=\"file\" name=\"filename[]\" multiple=\"multiple\" class=\"form-control\" autocomplete=\"off\" required>\n","            </p>\n","        </dl>\n","        <p>\n","        <br><br>\n","        <div style = \"text-align:center\">\n","            <input type=\"submit\" value=\"Submit\" class=\"btn btn-info\">\n","            </div?\n","        </p>\n","    </form>\n","</div>\n","</div>\n","\n","        </form>\n","    </body>\n","</html>\n","'''\n","file = open(\"templates/upload.html\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkT4xFHnJHtX"},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Multi-Media Computing Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>\n","<script src=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\"></script>\n","<script src=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.js\"></script>\n","<link href=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.css\" rel=\"stylesheet\">\n","</head>\n","\n","\n","    <body>\n","    <div class=\"container\">\n","        <h2 align=\"center\"> Style Transfer Results - Animation ver</h2>\n","        <br><br>\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Backgound Image\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Human Image\n","</div>\n","  </div>\n","\n","<br>\n","\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src=\"{{data1}}\" alt=\"background_image\"  width=\"400\">\n","{% endif %}\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src={{data2}} alt=\"human_image\" width=\"256\">\n","{% endif %}\n","</div>\n","</div>\n","\n","\n","\n","</div>\n","<br><br><br>\n","\n","\n","<div class=\"container\">\n","<div class=\"row\">\n","\n","    <form action = \"/Seg_human\" method = \"POST\">\n","      <input type = \"hidden\" name = \"test2\" value = {{file_name}} />        \n","       </select>\n","      </div>\n","      <div style = \"text-align:center\">\n","            <input type=\"submit\" name='msg' value= \"Segmentation\" class=\"btn btn-info\">\n","        </div>\n","        \n","    </form>\n","\n","</div>\n","</div>\n","\n","    </body>\n","</html>\n","'''\n","\n","file = open(\"templates/showAnime.html\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSLsgohAs6fd"},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Multi-Media Computing Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>\n","<script src=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\"></script>\n","<script src=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.js\"></script>\n","<link href=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.css\" rel=\"stylesheet\">\n","</head>\n","\n","\n","    <body>\n","    <div class=\"container\">\n","        <h2 align=\"center\"> Human Segmentation Results - Animation ver</h2>\n","        <br><br>\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Backgound Image\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","Human Segmentation Image\n","</div>\n","  </div>\n","\n","<br>\n","\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src=\"{{data1}}\" alt=\"background_image\"  width=\"400\">\n","{% endif %}\n","</div>\n","\n","<div style=\"float: left; padding: 10px; width: 50%;text-align:center\">\n","{% if True %}\n","\t<img src={{data2}} alt=\"human_image\" width=\"256\">\n","{% endif %}\n","</div>\n","</div>\n","\n","\n","\n","</div>\n","<br><br><br>\n","\n","\n","<div class=\"container\">\n","<div class=\"row\">\n","    <h4 align=\"center\">Select Position</h4>\n","\n","\n","    <form action = \"/Select_pos\" method = \"POST\">\n","      <div class=\"form-group\" style=\"margin: auto; text-align:center; width: 650\">\n","       <select name=\"position\" id=\"position\" class=\"form-control\">\n","          <option value=\"\">위치 선택</option>\n","          <option value=\"1\">1</option>\n","          <option value=\"2\">2</option>\n","          <option value=\"3\">3</option>\n","          <option value=\"4\">4</option>\n","          <option value=\"5\">5</option>\n","          <option value=\"6\">6</option>\n","          <option value=\"7\">7</option>\n","          <option value=\"8\">8</option>\n","       </select>\n","       <input type = \"hidden\" name = \"test2\" value = {{file_name}} /> \n","      <Br><br><br>\n","\n","            <input type=\"submit\" value=\"Submit\" class=\"btn btn-info\">\n","        </div>\n","    </form>\n","\n","</div>\n","</div>\n","\n","    </body>\n","</html>\n","'''\n","\n","file = open(\"templates/showSeg.html\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFjzwgSCBjKQ"},"source":["text = '''\n","<html>\n","\n","<head>\n","<title>Multi-Media Computing Website</title>\n","<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" />        \n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js\"></script>\n","<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>\n","<script src=\"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\"></script>\n","<script src=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.js\"></script>\n","<link href=\"http://rawgithub.com/indrimuska/jquery-editable-select/master/dist/jquery-editable-select.min.css\" rel=\"stylesheet\">\n","</head>\n","\n","\n","    <body>\n","    <div class=\"container\">\n","        <h2 align=\"center\"> Our Result</h2>\n","        <br>\n","\n","<br>\n","\n","\n","  <div class=\"container; text-align:center\">\n","<div style=\"float: left; padding: 10px; width: 100%;text-align:center\">\n","{% if True %}\n","\t<img src=\"{{data1}}\" alt=\"our_result_image\"  width=\"700\">\n","{% endif %}\n","</div>\n","\n","</div>\n","\n","\n","\n","</div>\n","\n","    </body>\n","</html>\n","'''\n","\n","file = open(\"templates/showResult.html\",\"w\")\n","file.write(text)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBcY4HH7Gwkb"},"source":["# flask 코드"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwGgZsMSJg2J","executionInfo":{"status":"ok","timestamp":1623138856817,"user_tz":-540,"elapsed":333,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"86711256-d537-4075-ba6d-c908acc645fb"},"source":["%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZZGRy2rQJHqw","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1623142185245,"user_tz":-540,"elapsed":354,"user":{"displayName":"서민균","photoUrl":"","userId":"05516558711386350040"}},"outputId":"ac1f5a6d-77f1-4477-eab2-fe241ec3dc85"},"source":["from flask import Flask, render_template, request\n","from werkzeug.utils import secure_filename\n","import shutil\n","app = Flask(__name__)\n","run_with_ngrok(app)   \n","\n","\n","# 파일 업로드 처리\n","@app.route('/fileUpload', methods = ['GET', 'POST'])\n","def upload_file():\n","    if request.method == 'POST':\n","        files = request.files\n","        file_names = []\n","        print(type(files))\n","        print('-----------')\n","        for f in files.to_dict(flat=False)['filename[]']:\n","            f.save('static/inputs/' + secure_filename(f.filename))\n","            print(f.filename)\n","            print('파일 업로드 성공!(' ,secure_filename(f.filename) ,')')\n","            file_names.append(secure_filename(f.filename))\n","        pad_msg = make_paddding('static/inputs/' + file_names[1])\n","        print(pad_msg)\n","        return style_transfer(file_names)\n","\n","\n","# 업로드 HTML 렌더링\n","@app.route('/')\n","def render_file():\n","    return render_template('upload.html')\n","    \n","\n","\n","@app.route('/Select_pos', methods = ['GET', 'POST'])\n","def select_position():\n","  human_file = request.form.get('test2')\n","  select = request.form.get('position')\n","  splicing_msg = splicing_back(int(select), back_file_path, human_file)\n","  print(splicing_msg)\n","  return render_template('showResult.html', data1 = 'static/splicing_outputs/splicing_result.jpg')\n","\n","\n","@app.route('/Seg_human', methods = ['GET', 'POST'])\n","def seg_human():\n","  human_file = request.form.get('test2')\n","\n","  seg_msg = segmentation(human_file)\n","  print(seg_msg)\n","  \n","  pro_msg, seg_file_path = pre_processing(human_file.split('.')[0] + '_black.png', human_file)\n","  print(pro_msg)\n","\n","  return render_template('showSeg.html', data1 = back_file_path, data2 = seg_file_path, file_name = human_file)\n","\n","\n","\n","\n","def style_transfer(file_names):\n","  execfile('/content/animegan2-pytorch/anime_execute.py')\n","  print(file_names)\n","  global back_file_path\n","  back_file_path = 'static/anime_outputs/' + file_names[0]\n","  human_file_path = 'static/anime_outputs/' + file_names[1]\n","  msg = file_names[1]\n","\n","  return render_template('showAnime.html', data1 = back_file_path, data2 = human_file_path, file_name = msg)\n","  #return \"style_trnasfer OK\"\n","\n","\n","if __name__ == '__main__':\n","    # 서버 실행\n","    app.run()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1b9248eb7ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_with_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'run_with_ngrok' is not defined"]}]},{"cell_type":"code","metadata":{"id":"MwWI8DL3hcCH"},"source":[""],"execution_count":null,"outputs":[]}]}